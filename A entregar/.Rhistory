source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
2*3-1
0.1+0.3-0.4
10^34+1-10^34
2*3-1
0.1+0.3-0.4
10^34+1-10^34
2*3-1
0.1+0.3-0.4
10^34+1-10^34
10^34+1-10^34
load("C:/Archivos de la facultad/Tecnicatura en Programacion/6° CUATRIMESTRE/INFERENCIA ESTADISTICA/clase 1/.RData")
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls())
setwd("C:\Archivos de la facultad\Tecnicatura en Programacion\6° CUATRIMESTRE\INFERENCIA ESTADISTICA\clase 1")
rm(list=ls())
```setwd("C:/Archivos de la facultad/Tecnicatura en Programacion/6° CUATRIMESTRE/INFERENCIA ESTADISTICA/clase 1")
rm(list=ls())
```setwd("C:/Archivos de la facultad/Tecnicatura en Programacion/6° CUATRIMESTRE/INFERENCIA ESTADISTICA/clase 1")
rm(list=ls())
```setwd("C:/Archivos de la facultad/Tecnicatura en Programacion/6° CUATRIMESTRE/INFERENCIA ESTADISTICA/clase 1")
rm(list=ls())
```setwd("C:/Archivos de la facultad/Tecnicatura en Programacion/6° CUATRIMESTRE/INFERENCIA ESTADISTICA/clase 1")
rm(list=ls())
```setwd("C:/Archivos de la facultad/Tecnicatura en Programacion/6° CUATRIMESTRE/INFERENCIA ESTADISTICA/clase 1")
rm(list=ls())
```setwd("C:/Archivos de la facultad/Tecnicatura en Programacion/6° CUATRIMESTRE/INFERENCIA ESTADISTICA/clase 1")
rm(list=ls())
```setwd("C:/Archivos de la facultad/Tecnicatura en Programacion/6° CUATRIMESTRE/INFERENCIA ESTADISTICA/clase 1")
rm(list=ls())
```setwd("C:/Archivos de la facultad/Tecnicatura en Programacion/6° CUATRIMESTRE/INFERENCIA ESTADISTICA/clase 1")
rm(list=ls())
```setwd("C:\Users\Matias\OneDrive\Escritorio\INFERENCIA ESTADISTICA\clase 1")
rm(list=ls())
```setwd("C:\Users\Matias\OneDrive\Escritorio\INFERENCIA ESTADISTICA\clase 1")
rm(list=ls())
```setwd("C:\Users\Matias\OneDrive\Escritorio\INFERENCIA ESTADISTICA\clase")
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls())
setwd("C:\Archivos de la facultad\Tecnicatura en Programacion\6° CUATRIMESTRE\INFERENCIA ESTADISTICA\clase 1")
rm(list=ls())
setwd("C:\Users\Matias\OneDrive\Escritorio\INFERENCIA ESTADISTICA\clase 1")
rm(list=ls())
setwd("C:/Users/Matias/OneDrive/Escritorio/INFERENCIA ESTADISTICA/clase 1")
rm(list=ls())
setwd("C:/Users/Matias/OneDrive/Escritorio/INFERENCIA ESTADISTICA/clase 1")
rm(list=ls())
setwd("C:/Users/Matias/OneDrive/Escritorio/INFERENCIA ESTADISTICA/clase 1")
datosTitanic <- read.csv("datos_titanic.csv")
# Miramos los nombres de las variables
names(datosTitanic)
head(datosTitanic)
attach(datosTitanic)
class(Survived)
class(Pclass)
class(Sex)
class(Age)
Survived <- as.factor(Survived)
class(Survived)
Pclass <- as.factor(Pclass)
class(Pclass)
table(Sex)
table(Survived)
tablaSexo <- table(Sex)
barplot(tablaSexo,width=0.02,xlim=c(0,0.15),col="blue",density=8)
tablaSobre <- table(Survived)/sum(table(Survived)) ## acá tenemos la tabla de frecuencias relativas
barplot(tablaSobre,width=0.02,xlim=c(0,0.15),col="red")
tablaClase <- table(Pclass)
tablaClase
porcentajes <- as.numeric(round(((prop.table(tablaClase))*100),2))
etiquetas <- c("Clase 1", "Clase 2", "Clase 3")
etiquetas <- paste(etiquetas, porcentajes)
etiquetas <- paste(etiquetas, "%", sep = "")
pie(porcentajes,etiquetas,col=c("blue","green","red"),main="Gráfico de Torta - Clases")
contingencia <- table(Sex,Pclass)
contingencia
barplot(contingencia,width=0.02,xlim=c(0,0.15),col= c("blue","red"),main="Sexo por Clase",legend = rownames(contingencia),args.legend = list(x = "topleft") )
barplot(contingencia,beside=TRUE,col= c("blue","red"),main="Sexo por Clase",legend = rownames(contingencia),args.legend = list(x = "topleft") )
contingencia2 <- table(Pclass,Sex)/sum(table(Pclass,Sex)) ## frecuencias relativas
barplot(contingencia2,width=0.02,xlim=c(0,0.15),col=c("blue","red","green"),main="Clase por Sexo",legend = rownames(contingencia2),args.legend = list(x = "topleft") )
mosaicplot(contingencia,color=2:4)
mosaicplot(t(contingencia),color=2:3)
par(mfrow=c(1,2))
hist(Fare,probability = TRUE,main="")
hist(Age,probability = TRUE,main="")
datos <- c(-2.78, -1, 4.4, -0.52, 0.02, -1.77, -0.12, -1.07, -2.54, 1.48, 7.8)
quantile(datos,0.25)
quantile(datos,0.75)
quantile(datos,0.25,type=1)
quantile(datos,0.75,type=1)
boxplot(datos)
par(mfrow=c(1,2))
boxplot(Fare,main="Tarifa")
boxplot(Age,main="Edad")
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls())
setwd("C:/Users/Matias/OneDrive/Escritorio/INFERENCIA ESTADISTICA/clase 1")
datosTitanic <- read.csv("datos_titanic.csv")
# Miramos los nombres de las variables
names(datosTitanic)
head(datosTitanic)
attach(datosTitanic)
class(Survived)
class(Pclass)
class(Sex)
class(Age)
Survived <- as.factor(Survived)
class(Survived)
Pclass <- as.factor(Pclass)
class(Pclass)
table(Sex)
table(Survived)
tablaSexo <- table(Sex)
barplot(tablaSexo,width=0.02,xlim=c(0,0.15),col="blue",density=8)
tablaSobre <- table(Survived)/sum(table(Survived)) ## acá tenemos la tabla de frecuencias relativas
barplot(tablaSobre,width=0.02,xlim=c(0,0.15),col="red")
tablaClase <- table(Pclass)
tablaClase
porcentajes <- as.numeric(round(((prop.table(tablaClase))*100),2))
etiquetas <- c("Clase 1", "Clase 2", "Clase 3")
etiquetas <- paste(etiquetas, porcentajes)
etiquetas <- paste(etiquetas, "%", sep = "")
pie(porcentajes,etiquetas,col=c("blue","green","red"),main="Gráfico de Torta - Clases")
contingencia <- table(Sex,Pclass)
contingencia
barplot(contingencia,width=0.02,xlim=c(0,0.15),col= c("blue","red"),main="Sexo por Clase",legend = rownames(contingencia),args.legend = list(x = "topleft") )
# Graficar dispersión y recta de regresión
plot(datos$horas, datos$notas,
main = "Regresión Lineal Simple",
xlab = "Horas de Estudio",
ylab = "Nota de Examen",
pch = 19, col = "blue")
notas <- c(50, 65, 70, 80, 90)   # Variable dependiente (Y)
# Crear un data frame
datos <- data.frame(horas, notas)
# Ajustar modelo de regresión lineal
modelo <- lm(notas ~ horas, data = datos)
horas <- c(2, 4, 6, 8, 10)       # Variable independiente (X)
notas <- c(50, 65, 70, 80, 90)   # Variable dependiente (Y)
# Datos
horas <- c(2, 4, 6, 8, 10)       # Variable independiente (X)
notas <- c(50, 65, 70, 80, 90)   # Variable dependiente (Y)
# Crear un data frame
datos <- data.frame(horas, notas)
# Ajustar modelo de regresión lineal
modelo <- lm(notas ~ horas, data = datos)
# Ver resultados
summary(modelo)
# Graficar dispersión y recta de regresión
plot(datos$horas, datos$notas,
main = "Regresión Lineal Simple",
xlab = "Horas de Estudio",
ylab = "Nota de Examen",
pch = 19, col = "blue")
abline(modelo, col = "red", lwd = 2)  # Agregar la recta de regresión
datos <- c(-2.78, -1, 4.4, -0.52, 0.02, -1.77, -0.12, -1.07, -2.54, 1.48, 7.8)
quantile(datos,0.25)
quantile(datos,0.75)
quantile(datos,0.25,type=1)
quantile(datos,0.75,type=1)
boxplot(datos)
abline(modelo1, col = "red", lwd = 2)
# Cargar datos
library(ISLR2)
# Cargar datos
library(ISLR2)
# Cargar datos
library(ISLR2)
data("Advertising")
# Modelo de regresión lineal simple
modelo1 <- lm(sales ~ TV, data = Advertising)
# Cargar datos
library(ISLR2)
data("Advertising")
# Modelo de regresión lineal simple
modelo1 <- lm(sales ~ TV, data = Advertising)
# Cargar datos
library(ISLR2)
data("Advertising")
# Cargar datos
library(ISLR2)
data("Advertising")
library(ISLR2)
data("Advertising")
# Cargar datos
library(ISLR2)
data("Advertising")
# Modelo de regresión lineal simple
modelo1 <- lm(sales ~ TV, data = Advertising)
# Cargar datos
library(ISLR2)
data("Advertising")
# Modelo de regresión lineal simple
modelo1 <- lm(sales ~ TV, data = Advertising)
# Cargar datos
library(ISLR2)
data("Advertising")
# Modelo de regresión lineal simple
modelo1 <- lm(sales ~ TV, data = Advertising)
# Cargar datos
library(ISLR2)
data("Advertising")
# Modelo de regresión lineal simple
modelo1 <- lm(sales ~ TV, data = Advertising)
# Gráfico con línea de regresión
plot(Advertising$TV, Advertising$sales,
pch = 19, col = "blue",
xlab = "Inversión en TV",
ylab = "Ventas")
abline(modelo1, col = "red", lwd = 2)
# Cargar datos
library(ISLR2)
data("Advertising")
# Modelo de regresión lineal simple
modelo1 <- lm(sales ~ TV, data = Advertising)
summary(modelo1)
# Modelo de regresión lineal múltiple
modelo2 <- lm(sales ~ TV + Radio + Newspaper, data = Advertising)
summary(modelo2)
# Gráfico con línea de regresión
plot(Advertising$TV, Advertising$sales,
pch = 19, col = "blue",
xlab = "Inversión en TV",
ylab = "Ventas")
abline(modelo1, col = "red", lwd = 2)
# Cargar datos
library(ISLR2)
data("Advertising")
# Modelo de regresión lineal simple
modelo1 <- lm(sales ~ TV, data = Advertising)
abline(modelo, col = "red", lwd = 2)  # Agregar la recta de regresión
#Cargo las librerias
library(tidyverse)
library(corrplot)
library(glmnet)
library(GGally)
#Empezamos borrando todas las variables
rm(list = ls())
#Cargamos el dataset
vinos <- read.csv2("winequality-red.csv",dec=".")
setwd("C:/Users/Matias/OneDrive/Escritorio/INFERENCIA ESTADISTICA/TP 1/A entregar")
#Miramos los nombres de las variables y vemos una vista rapida de la estructura
#del dataset
names(vinos)
head(vinos)
attach(vinos)
#Cargo las librerias
library(tidyverse)
library(corrplot)
library(glmnet)
library(GGally)
#Empezamos borrando todas las variables
rm(list = ls())
#Cargamos el dataset
vinos <- read.csv2("winequality-red.csv",dec=".")
#Miramos los nombres de las variables y vemos una vista rapida de la estructura
#del dataset
names(vinos)
head(vinos)
attach(vinos)
# EstadC-sticas descriptivas de cada variable
describe <- summary(vinos)
print(describe)
# Matriz de correlaciC3n y dispersiC3n
corrplot(cor(vinos), method = 'color', tl.cex = 0.7)
cor_matrix <- cor(vinos)
ggpairs(vinos, columns = 1:12, title = "Matriz de correlaciones y dispersiC3n")
# Matriz de correlaciC3n y dispersión
corrplot(cor(vinos), method = 'color', tl.cex = 0.7)
cor_matrix <- cor(vinos)
ggpairs(vinos, columns = 1:12, title = "Matriz de correlaciones y dispersiC3n")
# Matriz de correlaciC3n y dispersión
corrplot(cor(vinos), method = 'color', tl.cex = 0.7)
cor_matrix <- cor(vinos)
ggpairs(vinos, columns = 1:12, title = "Matriz de correlaciones y dispersión")
cor(vinos$quality, vinos[, 1:12])
# Separamos las variables explicativas de la variable respuesta
X <- vinos[, !(names(vinos) %in% c('quality'))]
y <- vinos$quality
# Armamos los conjuntos de entrenamiento y prueba
# En una relacion 80/20
set.seed(1)
train_idx <- sample(seq_len(nrow(vinos)), size = 0.8 * nrow(vinos))
train <- vinos[train_idx, ]
test <- vinos[-train_idx, ]
#Creo la matriz de diseC1o y vector de respuestas
X_train <- as.matrix(train[, !(names(train) %in% c('quality'))])
y_train <- train$quality
X_test <- as.matrix(test[, !(names(test) %in% c('quality'))])
y_test <- test$quality
# --- RegresiC3n lineal mC:ltiple ---
lm_fit <- lm(quality ~ ., data = train)
# Hacemos un summary del modelo
summary(lm_fit)
# Calculamos los predichos del conjunto de prueba
lm_pred <- predict(lm_fit, newdata = test)
# Metricas de evaluaciC3n del modelo
cat('RegresiC3n lineal mC:ltiple:\n')
cat('Error CuadrC!tico Medio (MSE):', mean((test$quality - lm_pred)^2), '\n')
cat('R2:', summary(lm_fit)$r.squared, '\n')
residuos <- resid(lm_fit)
ajustados <- fitted(lm_fit)
#Histograma de Residuos
hist(residuos,
breaks = 30,
col = "steelblue",
main = "DistribuciC3n de residuos",
xlab = "Residuos")
# GrC!fico de dispersiC3n
plot(ajustados, residuos,
xlab = "Valores ajustados",
ylab = "Residuos",
main = "Residuos vs Valores ajustados",
pch = 20, col = "darkred")
abline(h = 0, lty = 2, col = "gray")
# --- ComparaciC3n de residuos: Boxplot de los tres modelos ---
residuos_df <- data.frame(
Residuo = c(as.numeric(residuos), as.numeric(residuos_ridge), as.numeric(residuos_lasso)),
Modelo = factor(
rep(c("Lineal", "Ridge", "LASSO"),
times = c(length(residuos), length(residuos_ridge), length(residuos_lasso)))
)
)
boxplot(Residuo ~ Modelo, data = residuos_df,
col = c("steelblue", "tomato", "orange"),
main = "ComparaciC3n de residuos por modelo",
ylab = "Residuos",
xlab = "Modelo")
cv_ridge <- cv.glmnet(X_train, y_train, alpha = 0)
best_lambda_ridge <- cv_ridge$lambda.min
ridge_pred <- predict(cv_ridge, s = best_lambda_ridge, newx = X_test)
# Metricas de evaluaciC3n del modelo
cat('\nRidge:\n')
#Cargo las librerias
library(tidyverse)
library(corrplot)
library(glmnet)
library(GGally)
#Empezamos borrando todas las variables
rm(list = ls())
#Cargamos el dataset
vinos <- read.csv2("winequality-red.csv",dec=".")
#Miramos los nombres de las variables y vemos una vista rapida de la estructura del dataset
names(vinos)
head(vinos)
attach(vinos)
# Estadisticas descriptivas de cada variable
describe <- summary(vinos)
print(describe)
# Matriz de correlación y dispersión
corrplot(cor(vinos), method = 'color', tl.cex = 0.7)
cor_matrix <- cor(vinos)
ggpairs(vinos, columns = 1:12, title = "Matriz de correlaciones y dispersiC3n")
cor(vinos$quality, vinos[, 1:12])
cor(vinos$quality, vinos[, 1:12])
# Separamos las variables explicativas de la variable respuesta
X <- vinos[, !(names(vinos) %in% c('quality'))]
y <- vinos$quality
# Armamos los conjuntos de entrenamiento y prueba
# En una relacion 80/20
set.seed(1)
train_idx <- sample(seq_len(nrow(vinos)), size = 0.8 * nrow(vinos))
train <- vinos[train_idx, ]
test <- vinos[-train_idx, ]
#Creo la matriz de diseño y vector de respuestas
X_train <- as.matrix(train[, !(names(train) %in% c('quality'))])
y_train <- train$quality
X_test <- as.matrix(test[, !(names(test) %in% c('quality'))])
y_test <- test$quality
# --- Regresión lineal múltiple ---
lm_fit <- lm(quality ~ ., data = train)
# Hacemos un summary del modelo
summary(lm_fit)
# Calculamos los predichos del conjunto de prueba
lm_pred <- predict(lm_fit, newdata = test)
# Metricas de evaluación del modelo
cat('RegresiC3n lineal mC:ltiple:\n')
cat('Error CuadrC!tico Medio (MSE):', mean((test$quality - lm_pred)^2), '\n')
cat('R2:', summary(lm_fit)$r.squared, '\n')
residuos <- resid(lm_fit)
ajustados <- fitted(lm_fit)
#Histograma de Residuos
hist(residuos,
breaks = 30,
col = "steelblue",
main = "DistribuciC3n de residuos",
xlab = "Residuos")
# GrC!fico de dispersiC3n
plot(ajustados, residuos,
xlab = "Valores ajustados",
ylab = "Residuos",
main = "Residuos vs Valores ajustados",
pch = 20, col = "darkred")
abline(h = 0, lty = 2, col = "gray")
cv_ridge <- cv.glmnet(X_train, y_train, alpha = 0)
best_lambda_ridge <- cv_ridge$lambda.min
ridge_pred <- predict(cv_ridge, s = best_lambda_ridge, newx = X_test)
cv_ridge <- cv.glmnet(X_train, y_train, alpha = 0)
best_lambda_ridge <- cv_ridge$lambda.min
ridge_pred <- predict(cv_ridge, s = best_lambda_ridge, newx = X_test)
# Metricas de evaluación del modelo
cat('\nRidge:\n')
r2_ridge <- 1 - sum((y_test - ridge_pred)^2) / sum((y_test - mean(y_test))^2)
cat("Error CuadrC!tico Medio (MSE): ", mean((y_test - ridge_pred)^2))
cat('R2:', r2_ridge, '\n')
# Graficos de residuos
residuos_ridge <- y_test-ridge_pred
# Histograma
hist(residuos_ridge,
breaks = 30,
col = "tomato",
main = "Residuos del modelo penalizado",
xlab = "Residuos")
# Valores ajustados vs Residuos
plot(ridge_pred, residuos_ridge,
xlab = "Valores ajustados",
ylab = "Residuos",
main = "Residuos vs Valores ajustados (penalizado)",
pch = 20, col = "blue")
abline(h = 0, lty = 2, col = "gray")
# --- LASSO ---
cv_lasso <- cv.glmnet(X_train, y_train, alpha = 1)
best_lambda_lasso <- cv_lasso$lambda.min
lasso_pred <- predict(cv_lasso, s = best_lambda_lasso, newx = X_test)
# Metricas de evaluación del modelo
r2_lasso <- 1 - sum((y_test - lasso_pred)^2) / sum((y_test - mean(y_test))^2)
cat('\nLASSO:\n')
cat("Error CuadrC!tico Medio (MSE): ", mean((y_test - lasso_pred)^2))
cat('R2:', r2_lasso, '\n')
# Graficos de residuos
residuos_lasso <- y_test-lasso_pred
# Histograma
hist(residuos_lasso,
breaks = 30,
col = "tomato",
main = "Residuos del modelo penalizado",
xlab = "Residuos")
# Valores ajustados vs Residuos
plot(lasso_pred, residuos_lasso,
xlab = "Valores ajustados",
ylab = "Residuos",
main = "Residuos vs Valores ajustados (penalizado)",
pch = 20, col = "blue")
abline(h = 0, lty = 2, col = "gray")
# --- Comparación de residuos: Boxplot de los tres modelos ---
residuos_df <- data.frame(
Residuo = c(as.numeric(residuos), as.numeric(residuos_ridge), as.numeric(residuos_lasso)),
Modelo = factor(
rep(c("Lineal", "Ridge", "LASSO"),
times = c(length(residuos), length(residuos_ridge), length(residuos_lasso)))
)
)
boxplot(Residuo ~ Modelo, data = residuos_df,
col = c("steelblue", "tomato", "orange"),
main = "Comparación de residuos por modelo",
ylab = "Residuos",
xlab = "Modelo")
# --- Comparación de residuos: Boxplot de los tres modelos ---
residuos_df <- data.frame(
Residuo = c(as.numeric(residuos), as.numeric(residuos_ridge), as.numeric(residuos_lasso)),
Modelo = factor(
rep(c("Lineal", "Ridge", "LASSO"),
times = c(length(residuos), length(residuos_ridge), length(residuos_lasso)))
)
)
boxplot(Residuo ~ Modelo, data = residuos_df,
col = c("steelblue", "tomato", "orange"),
main = "Comparación de residuos por modelo",
ylab = "Residuos",
xlab = "Modelo")
# --- Boxplot 1: Valores predichos por modelo ---
predichos_df <- data.frame(
Predicho = c(as.numeric(lm_pred), as.numeric(ridge_pred), as.numeric(lasso_pred)),
Modelo = factor(
rep(c("Lineal", "Ridge", "LASSO"),
times = c(length(lm_pred), length(ridge_pred), length(lasso_pred)))
)
)
boxplot(Predicho ~ Modelo, data = predichos_df,
col = c("steelblue", "tomato", "orange"),
main = "Comparación de valores predichos por modelo",
ylab = "Valor predicho",
xlab = "Modelo")
# --- Boxplot 2: Residuos absolutos por modelo ---
residuos_abs_df <- data.frame(
ResiduoAbs = c(abs(as.numeric(residuos)), abs(as.numeric(residuos_ridge)), abs(as.numeric(residuos_lasso))),
Modelo = factor(
rep(c("Lineal", "Ridge", "LASSO"),
times = c(length(residuos), length(residuos_ridge), length(residuos_lasso)))
)
)
boxplot(ResiduoAbs ~ Modelo, data = residuos_abs_df,
col = c("steelblue", "tomato", "orange"),
main = "Comparación de residuos absolutos por modelo",
ylab = "|Residuo|",
xlab = "Modelo")
# --- Boxplot 3: Calidad real vs valores predichos por modelo ---
calidad_predicho_df <- data.frame(
Real = rep(y_test, 3),
Predicho = c(as.numeric(lm_pred), as.numeric(ridge_pred), as.numeric(lasso_pred)),
Modelo = factor(
rep(c("Lineal", "Ridge", "LASSO"),
each = length(y_test))
)
)
boxplot(Predicho ~ Real + Modelo, data = calidad_predicho_df,
col = c("steelblue", "tomato", "orange"),
main = "Valores predichos agrupados por calidad real y modelo",
ylab = "Valor predicho",
xlab = "Calidad real (por modelo)",
las = 2)
