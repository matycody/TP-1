library(ggplot2)
library(dplyr)
library(maps)
meteoritos_limpios <- meteoritos %>%
filter(!is.na(long) & !is.na(lat) & !is.na(mass)) %>%
mutate(mass_toneladas = mass / 1000)
ggplot(meteoritos_limpios, aes(x = long, y = lat)) +
borders("world", colour = "black", fill = "grey80") +
geom_point(aes(size = mass_toneladas, color = mass_toneladas), alpha = 0.8) +
scale_size_continuous(range = c(2, 10), name = "Masa (toneladas)") +
scale_color_gradient(low = "yellow", high = "red", name = "Masa (toneladas)") +
scale_x_continuous(breaks = seq(-180, 180, by = 30), name = "Longitud") +
scale_y_continuous(breaks = seq(-90, 90, by = 15), name = "Latitud") +
labs(
title = "Mapa de Impacto de Meteoritos según Masa",
caption = "Fuente: NASA"
) +
theme_minimal() +
theme(
plot.title = element_text(hjust = 0.5),
legend.position = "right"
)
2x3-1
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
0.1+0.3-0.4
10^34+1-10^34
0.1+0.3-0.4
10^34+1-10^34
0.1+0.3-0.4
10^34+1-10^34
0.1+0.3-0.4
10^34+1-10^34
0.1+0.3-0.4
10^34+1-10^34
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
2*3-1
0.1+0.3-0.4
10^34+1-10^34
2*3-1
0.1+0.3-0.4
10^34+1-10^34
2*3-1
0.1+0.3-0.4
10^34+1-10^34
10^34+1-10^34
load("C:/Archivos de la facultad/Tecnicatura en Programacion/6° CUATRIMESTRE/INFERENCIA ESTADISTICA/clase 1/.RData")
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls())
setwd("C:\Archivos de la facultad\Tecnicatura en Programacion\6° CUATRIMESTRE\INFERENCIA ESTADISTICA\clase 1")
rm(list=ls())
```setwd("C:/Archivos de la facultad/Tecnicatura en Programacion/6° CUATRIMESTRE/INFERENCIA ESTADISTICA/clase 1")
rm(list=ls())
```setwd("C:/Archivos de la facultad/Tecnicatura en Programacion/6° CUATRIMESTRE/INFERENCIA ESTADISTICA/clase 1")
rm(list=ls())
```setwd("C:/Archivos de la facultad/Tecnicatura en Programacion/6° CUATRIMESTRE/INFERENCIA ESTADISTICA/clase 1")
rm(list=ls())
```setwd("C:/Archivos de la facultad/Tecnicatura en Programacion/6° CUATRIMESTRE/INFERENCIA ESTADISTICA/clase 1")
rm(list=ls())
```setwd("C:/Archivos de la facultad/Tecnicatura en Programacion/6° CUATRIMESTRE/INFERENCIA ESTADISTICA/clase 1")
rm(list=ls())
```setwd("C:/Archivos de la facultad/Tecnicatura en Programacion/6° CUATRIMESTRE/INFERENCIA ESTADISTICA/clase 1")
rm(list=ls())
```setwd("C:/Archivos de la facultad/Tecnicatura en Programacion/6° CUATRIMESTRE/INFERENCIA ESTADISTICA/clase 1")
rm(list=ls())
```setwd("C:/Archivos de la facultad/Tecnicatura en Programacion/6° CUATRIMESTRE/INFERENCIA ESTADISTICA/clase 1")
rm(list=ls())
```setwd("C:\Users\Matias\OneDrive\Escritorio\INFERENCIA ESTADISTICA\clase 1")
rm(list=ls())
```setwd("C:\Users\Matias\OneDrive\Escritorio\INFERENCIA ESTADISTICA\clase 1")
rm(list=ls())
```setwd("C:\Users\Matias\OneDrive\Escritorio\INFERENCIA ESTADISTICA\clase")
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls())
setwd("C:\Archivos de la facultad\Tecnicatura en Programacion\6° CUATRIMESTRE\INFERENCIA ESTADISTICA\clase 1")
rm(list=ls())
setwd("C:\Users\Matias\OneDrive\Escritorio\INFERENCIA ESTADISTICA\clase 1")
rm(list=ls())
setwd("C:/Users/Matias/OneDrive/Escritorio/INFERENCIA ESTADISTICA/clase 1")
rm(list=ls())
setwd("C:/Users/Matias/OneDrive/Escritorio/INFERENCIA ESTADISTICA/clase 1")
rm(list=ls())
setwd("C:/Users/Matias/OneDrive/Escritorio/INFERENCIA ESTADISTICA/clase 1")
datosTitanic <- read.csv("datos_titanic.csv")
# Miramos los nombres de las variables
names(datosTitanic)
head(datosTitanic)
attach(datosTitanic)
class(Survived)
class(Pclass)
class(Sex)
class(Age)
Survived <- as.factor(Survived)
class(Survived)
Pclass <- as.factor(Pclass)
class(Pclass)
table(Sex)
table(Survived)
tablaSexo <- table(Sex)
barplot(tablaSexo,width=0.02,xlim=c(0,0.15),col="blue",density=8)
tablaSobre <- table(Survived)/sum(table(Survived)) ## acá tenemos la tabla de frecuencias relativas
barplot(tablaSobre,width=0.02,xlim=c(0,0.15),col="red")
tablaClase <- table(Pclass)
tablaClase
porcentajes <- as.numeric(round(((prop.table(tablaClase))*100),2))
etiquetas <- c("Clase 1", "Clase 2", "Clase 3")
etiquetas <- paste(etiquetas, porcentajes)
etiquetas <- paste(etiquetas, "%", sep = "")
pie(porcentajes,etiquetas,col=c("blue","green","red"),main="Gráfico de Torta - Clases")
contingencia <- table(Sex,Pclass)
contingencia
barplot(contingencia,width=0.02,xlim=c(0,0.15),col= c("blue","red"),main="Sexo por Clase",legend = rownames(contingencia),args.legend = list(x = "topleft") )
barplot(contingencia,beside=TRUE,col= c("blue","red"),main="Sexo por Clase",legend = rownames(contingencia),args.legend = list(x = "topleft") )
contingencia2 <- table(Pclass,Sex)/sum(table(Pclass,Sex)) ## frecuencias relativas
barplot(contingencia2,width=0.02,xlim=c(0,0.15),col=c("blue","red","green"),main="Clase por Sexo",legend = rownames(contingencia2),args.legend = list(x = "topleft") )
mosaicplot(contingencia,color=2:4)
mosaicplot(t(contingencia),color=2:3)
par(mfrow=c(1,2))
hist(Fare,probability = TRUE,main="")
hist(Age,probability = TRUE,main="")
datos <- c(-2.78, -1, 4.4, -0.52, 0.02, -1.77, -0.12, -1.07, -2.54, 1.48, 7.8)
quantile(datos,0.25)
quantile(datos,0.75)
quantile(datos,0.25,type=1)
quantile(datos,0.75,type=1)
boxplot(datos)
par(mfrow=c(1,2))
boxplot(Fare,main="Tarifa")
boxplot(Age,main="Edad")
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls())
setwd("C:/Users/Matias/OneDrive/Escritorio/INFERENCIA ESTADISTICA/clase 1")
datosTitanic <- read.csv("datos_titanic.csv")
# Miramos los nombres de las variables
names(datosTitanic)
head(datosTitanic)
attach(datosTitanic)
class(Survived)
class(Pclass)
class(Sex)
class(Age)
Survived <- as.factor(Survived)
class(Survived)
Pclass <- as.factor(Pclass)
class(Pclass)
table(Sex)
table(Survived)
tablaSexo <- table(Sex)
barplot(tablaSexo,width=0.02,xlim=c(0,0.15),col="blue",density=8)
tablaSobre <- table(Survived)/sum(table(Survived)) ## acá tenemos la tabla de frecuencias relativas
barplot(tablaSobre,width=0.02,xlim=c(0,0.15),col="red")
tablaClase <- table(Pclass)
tablaClase
porcentajes <- as.numeric(round(((prop.table(tablaClase))*100),2))
etiquetas <- c("Clase 1", "Clase 2", "Clase 3")
etiquetas <- paste(etiquetas, porcentajes)
etiquetas <- paste(etiquetas, "%", sep = "")
pie(porcentajes,etiquetas,col=c("blue","green","red"),main="Gráfico de Torta - Clases")
contingencia <- table(Sex,Pclass)
contingencia
barplot(contingencia,width=0.02,xlim=c(0,0.15),col= c("blue","red"),main="Sexo por Clase",legend = rownames(contingencia),args.legend = list(x = "topleft") )
# Graficar dispersión y recta de regresión
plot(datos$horas, datos$notas,
main = "Regresión Lineal Simple",
xlab = "Horas de Estudio",
ylab = "Nota de Examen",
pch = 19, col = "blue")
notas <- c(50, 65, 70, 80, 90)   # Variable dependiente (Y)
# Crear un data frame
datos <- data.frame(horas, notas)
# Ajustar modelo de regresión lineal
modelo <- lm(notas ~ horas, data = datos)
horas <- c(2, 4, 6, 8, 10)       # Variable independiente (X)
notas <- c(50, 65, 70, 80, 90)   # Variable dependiente (Y)
# Datos
horas <- c(2, 4, 6, 8, 10)       # Variable independiente (X)
notas <- c(50, 65, 70, 80, 90)   # Variable dependiente (Y)
# Crear un data frame
datos <- data.frame(horas, notas)
# Ajustar modelo de regresión lineal
modelo <- lm(notas ~ horas, data = datos)
# Ver resultados
summary(modelo)
# Graficar dispersión y recta de regresión
plot(datos$horas, datos$notas,
main = "Regresión Lineal Simple",
xlab = "Horas de Estudio",
ylab = "Nota de Examen",
pch = 19, col = "blue")
abline(modelo, col = "red", lwd = 2)  # Agregar la recta de regresión
datos <- c(-2.78, -1, 4.4, -0.52, 0.02, -1.77, -0.12, -1.07, -2.54, 1.48, 7.8)
quantile(datos,0.25)
quantile(datos,0.75)
quantile(datos,0.25,type=1)
quantile(datos,0.75,type=1)
boxplot(datos)
abline(modelo1, col = "red", lwd = 2)
# Cargar datos
library(ISLR2)
# Cargar datos
library(ISLR2)
# Cargar datos
library(ISLR2)
data("Advertising")
# Modelo de regresión lineal simple
modelo1 <- lm(sales ~ TV, data = Advertising)
# Cargar datos
library(ISLR2)
data("Advertising")
# Modelo de regresión lineal simple
modelo1 <- lm(sales ~ TV, data = Advertising)
# Cargar datos
library(ISLR2)
data("Advertising")
# Cargar datos
library(ISLR2)
data("Advertising")
library(ISLR2)
data("Advertising")
# Cargar datos
library(ISLR2)
data("Advertising")
# Modelo de regresión lineal simple
modelo1 <- lm(sales ~ TV, data = Advertising)
# Cargar datos
library(ISLR2)
data("Advertising")
# Modelo de regresión lineal simple
modelo1 <- lm(sales ~ TV, data = Advertising)
# Cargar datos
library(ISLR2)
data("Advertising")
# Modelo de regresión lineal simple
modelo1 <- lm(sales ~ TV, data = Advertising)
# Cargar datos
library(ISLR2)
data("Advertising")
# Modelo de regresión lineal simple
modelo1 <- lm(sales ~ TV, data = Advertising)
# Gráfico con línea de regresión
plot(Advertising$TV, Advertising$sales,
pch = 19, col = "blue",
xlab = "Inversión en TV",
ylab = "Ventas")
abline(modelo1, col = "red", lwd = 2)
# Cargar datos
library(ISLR2)
data("Advertising")
# Modelo de regresión lineal simple
modelo1 <- lm(sales ~ TV, data = Advertising)
summary(modelo1)
# Modelo de regresión lineal múltiple
modelo2 <- lm(sales ~ TV + Radio + Newspaper, data = Advertising)
summary(modelo2)
# Gráfico con línea de regresión
plot(Advertising$TV, Advertising$sales,
pch = 19, col = "blue",
xlab = "Inversión en TV",
ylab = "Ventas")
abline(modelo1, col = "red", lwd = 2)
# Cargar datos
library(ISLR2)
data("Advertising")
# Modelo de regresión lineal simple
modelo1 <- lm(sales ~ TV, data = Advertising)
abline(modelo, col = "red", lwd = 2)  # Agregar la recta de regresión
vinos <- read.csv2("winequality-red.csv",dec=".")
setwd("C:/Users/Matias/OneDrive/Escritorio/INFERENCIA ESTADISTICA/TP 1")
vinos <- read.csv2("winequality-red.csv",dec=".")
vinos <- read.csv2("winequality-red.csv",dec=".")
View(vinos)
# Cargar datos
wine <- read.csv2('winequality-red.csv', dec = ".")
# Estadísticas descriptivas
describe <- summary(wine)
print(describe)
# Matriz de correlación
library(corrplot)
corrplot(cor(wine), method = 'color', tl.cex = 0.7)
# Separar variables
X <- wine[, !(names(wine) %in% c('quality'))]
y <- wine$quality
# Train/test split
set.seed(42)
train_idx <- sample(seq_len(nrow(wine)), size = 0.8 * nrow(wine))
train <- wine[train_idx, ]
test <- wine[-train_idx, ]
# --- Regresión lineal múltiple ---
lm_fit <- lm(quality ~ ., data = train)
lm_pred <- predict(lm_fit, newdata = test)
cat('Regresión lineal múltiple:\n')
cat('RMSE:', sqrt(mean((test$quality - lm_pred)^2)), '\n')
cat('R2:', summary(lm_fit)$r.squared, '\n')
cat('Coeficientes:\n')
print(coef(lm_fit))
# --- Ridge ---
library(glmnet)
X_train <- as.matrix(train[, !(names(train) %in% c('quality'))])
y_train <- train$quality
X_test <- as.matrix(test[, !(names(test) %in% c('quality'))])
y_test <- test$quality
cv_ridge <- cv.glmnet(X_train, y_train, alpha = 0)
best_lambda_ridge <- cv_ridge$lambda.min
ridge_pred <- predict(cv_ridge, s = best_lambda_ridge, newx = X_test)
cat('\nRidge:\n')
cat('Mejor lambda:', best_lambda_ridge, '\n')
cat('RMSE:', sqrt(mean((y_test - ridge_pred)^2)), '\n')
# --- LASSO ---
cv_lasso <- cv.glmnet(X_train, y_train, alpha = 1)
best_lambda_lasso <- cv_lasso$lambda.min
lasso_pred <- predict(cv_lasso, s = best_lambda_lasso, newx = X_test)
cat('\nLASSO:\n')
cat('Mejor lambda:', best_lambda_lasso, '\n')
cat('RMSE:', sqrt(mean((y_test - lasso_pred)^2)), '\n')
# Gráficos de predicción vs real
par(mfrow = c(1,3))
plot(y_test, lm_pred, main = 'Lineal múltiple', xlab = 'Real', ylab = 'Predicho')
plot(y_test, ridge_pred, main = 'Ridge', xlab = 'Real', ylab = 'Predicho')
plot(y_test, lasso_pred, main = 'LASSO', xlab = 'Real', ylab = 'Predicho')
par(mfrow = c(1,1))
vinos <- read.csv2("winequality-red.csv",dec=".")
# Cargar datos
wine <- read.csv2('winequality-red.csv', dec = ".")
# Estadísticas descriptivas
describe <- summary(wine)
print(describe)
# Matriz de correlación
library(corrplot)
install.packages("corrplot")
vinos <- read.csv2("winequality-red.csv",dec=".")
# Cargar datos
wine <- read.csv2('winequality-red.csv', dec = ".")
# Estadísticas descriptivas
describe <- summary(wine)
print(describe)
# Matriz de correlación
library(corrplot)
corrplot(cor(wine), method = 'color', tl.cex = 0.7)
# Separar variables
X <- wine[, !(names(wine) %in% c('quality'))]
y <- wine$quality
# Train/test split
set.seed(42)
train_idx <- sample(seq_len(nrow(wine)), size = 0.8 * nrow(wine))
train <- wine[train_idx, ]
test <- wine[-train_idx, ]
# --- Regresión lineal múltiple ---
lm_fit <- lm(quality ~ ., data = train)
lm_pred <- predict(lm_fit, newdata = test)
cat('Regresión lineal múltiple:\n')
cat('RMSE:', sqrt(mean((test$quality - lm_pred)^2)), '\n')
cat('R2:', summary(lm_fit)$r.squared, '\n')
cat('Coeficientes:\n')
print(coef(lm_fit))
# --- Ridge ---
library(glmnet)
X_train <- as.matrix(train[, !(names(train) %in% c('quality'))])
y_train <- train$quality
X_test <- as.matrix(test[, !(names(test) %in% c('quality'))])
y_test <- test$quality
cv_ridge <- cv.glmnet(X_train, y_train, alpha = 0)
best_lambda_ridge <- cv_ridge$lambda.min
ridge_pred <- predict(cv_ridge, s = best_lambda_ridge, newx = X_test)
cat('\nRidge:\n')
cat('Mejor lambda:', best_lambda_ridge, '\n')
cat('RMSE:', sqrt(mean((y_test - ridge_pred)^2)), '\n')
# --- LASSO ---
cv_lasso <- cv.glmnet(X_train, y_train, alpha = 1)
best_lambda_lasso <- cv_lasso$lambda.min
lasso_pred <- predict(cv_lasso, s = best_lambda_lasso, newx = X_test)
cat('\nLASSO:\n')
cat('Mejor lambda:', best_lambda_lasso, '\n')
cat('RMSE:', sqrt(mean((y_test - lasso_pred)^2)), '\n')
# Gráficos de predicción vs real
par(mfrow = c(1,3))
plot(y_test, lm_pred, main = 'Lineal múltiple', xlab = 'Real', ylab = 'Predicho')
plot(y_test, ridge_pred, main = 'Ridge', xlab = 'Real', ylab = 'Predicho')
plot(y_test, lasso_pred, main = 'LASSO', xlab = 'Real', ylab = 'Predicho')
par(mfrow = c(1,1))
rm(list=ls()) ## limpia el historial, removiendo todas las variables
# cargo librerias, antes hay que instalarlas
library(ISLR)
library(plotmo)     # para graficar
library(glmnet)
#fix(Hitters)
names(Hitters)
dim(Hitters)
#Chequeo missings
sum(is.na(Hitters$Salary))
Hitters<- na.omit(Hitters)
sum(is.na(Hitters$Salary))
dim(Hitters)
# Ajustamos minimos cuadrados con todas las covariables
summary(lm(Salary~.,Hitters))
#Creamos la muestra de entrenamiento y de validacion
set.seed(1)
train <- sample(c(TRUE,FALSE), nrow(Hitters),rep=TRUE)
test <- (!train )
#Creo la matriz de diseño y vector de respuestas
x <- model.matrix(Salary~.,Hitters)[,-1]
y <- Hitters$Salary
y.test <- y[test]
grilla <- 10^seq(10,-2,length=100)
ridge.mod <- glmnet (x[train,],y[train],alpha =0, lambda =grilla) #por defecto estandariza variables
## Veo los coeficientes
dim(coef(ridge.mod))
ridge.mod$lambda
#Inpeccionamos para distintos valores de lambda
ridge.mod$lambda[90]
coef(lasso.mod)[,90]
ridge.mod$lambda[70]
coef(lasso.mod)[,70]
ridge.mod$lambda[30]
coef(lasso.mod)[,30]
plot(ridge.mod)
plot(ridge.mod,label=T,xvar="lambda")
#Ridge
set.seed(1)
cv.out <- cv.glmnet(x[train,],y[train],alpha =0)
plot(cv.out)
mejorlam <- cv.out$lambda.min
log(mejorlam)
#miro cuanto vale el MSE n la muestra de validacion
coef(ridge.mod,s=cv.out$lambda.min)
ridge.pred <- predict(ridge.mod,s=mejorlam,newx=x[test,])
mean((ridge.pred-y.test)^2)
#Tmb tenemos el lambda estimado de la regla de 1 desvio estandar
log(cv.out$lambda.1se)
coef(ridge.mod,s=cv.out$lambda.1se)
ridge.pred <- predict(ridge.mod,s=cv.out$lambda.1se,newx=x[test,])
mean((ridge.pred-y.test)^2)
# Calculo LASSO
grilla <- 10^seq(10,-2,length=100)
lasso.mod <- glmnet(x[train ,],y[train],alpha =1, lambda =grilla) # por defecto estandariza variables
## Veo los coeficientes
dim(coef(lasso.mod))
lasso.mod$lambda
#Inpeccionamos para distintos valores de lambda
lasso.mod$lambda[90]
coef(lasso.mod)[,90]
lasso.mod$lambda[70]
coef(lasso.mod)[,70]
lasso.mod$lambda[30]
coef(lasso.mod)[,30]
## Grafico
plot(lasso.mod)
plot(lasso.mod,label=T,xvar="lambda")
#Creo la matriz de diseño y vector de respuestas
x <- model.matrix(Salary~.,Hitters)[,-1]
y <- Hitters$Salary
y.test <- y[test]
#LASSO
set.seed(1)
cv.out <- cv.glmnet(x[train,],y[train],alpha =1)
plot(cv.out)
mejorlam <- cv.out$lambda.min
log(mejorlam)
#miro cuanto vale el ECM en la muestra de validacion
coef(lasso.mod,s=cv.out$lambda.min)
lasso.pred <- predict(lasso.mod,s=mejorlam,newx=x[test,])
mean((lasso.pred-y.test)^2)
#Tmb tenemos el lambda estimado de la regla de 1 desvio estandar
log(cv.out$lambda.1se)
coef(lasso.mod,s=cv.out$lambda.1se)
lasso.pred <- predict(lasso.mod,s=cv.out$lambda.1se,newx=x[test,])
mean((lasso.pred-y.test)^2)
